"""
Performance benchmarks and stress tests for the Protein Diffusion Design Lab.

These tests measure performance characteristics and identify bottlenecks
in the protein generation and analysis pipeline.
"""

import pytest
import time
import statistics
import psutil
import os
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.protein_diffusion import (
    ProteinDiffuser, ProteinDiffuserConfig,
    AffinityRanker, AffinityRankerConfig
)


class PerformanceMonitor:
    """Helper class for monitoring performance metrics."""
    
    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.start_time = None
        self.start_memory = None
    
    def start(self):
        """Start monitoring."""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss
    
    def stop(self) -> Dict[str, float]:
        """Stop monitoring and return metrics."""
        end_time = time.time()
        end_memory = self.process.memory_info().rss
        
        return {
            'elapsed_time': end_time - self.start_time,
            'memory_delta_mb': (end_memory - self.start_memory) / 1024 / 1024,
            'final_memory_mb': end_memory / 1024 / 1024,
            'cpu_percent': self.process.cpu_percent()
        }


@pytest.mark.benchmark
class TestGenerationPerformance:
    """Benchmark protein generation performance."""
    
    @pytest.fixture
    def base_config(self):
        """Base configuration for performance tests."""
        config = ProteinDiffuserConfig()
        config.enable_caching = False  # Disable for pure performance measurement
        return config
    
    def test_generation_speed_scaling(self, base_config):
        """Test how generation speed scales with number of samples."""
        sample_counts = [1, 5, 10, 20]
        results = {}
        
        diffuser = ProteinDiffuser(base_config)
        
        for num_samples in sample_counts:
            monitor = PerformanceMonitor()
            monitor.start()
            
            generation_results = diffuser.generate(
                num_samples=num_samples,
                max_length=64,
                progress=False
            )
            
            metrics = monitor.stop()
            metrics['sequences_generated'] = len(generation_results)
            metrics['time_per_sequence'] = metrics['elapsed_time'] / num_samples
            results[num_samples] = metrics
            
            print(f\"Generated {num_samples} sequences in {metrics['elapsed_time']:.2f}s \"\n                  f\"({metrics['time_per_sequence']:.3f}s per sequence)\")\n        \n        # Verify reasonable scaling\n        assert results[1]['time_per_sequence'] > 0\n        assert results[20]['elapsed_time'] > results[1]['elapsed_time']\n        \n        # Time per sequence shouldn't increase too much with batch size\n        single_time = results[1]['time_per_sequence']\n        batch_time = results[20]['time_per_sequence']\n        assert batch_time < single_time * 2  # Should be at most 2x slower per sequence\n    \n    def test_sequence_length_performance(self, base_config):\n        \"\"\"Test performance impact of sequence length.\"\"\"\n        lengths = [32, 64, 128, 256]\n        results = {}\n        \n        diffuser = ProteinDiffuser(base_config)\n        \n        for max_length in lengths:\n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            generation_results = diffuser.generate(\n                num_samples=5,\n                max_length=max_length,\n                progress=False\n            )\n            \n            metrics = monitor.stop()\n            metrics['avg_actual_length'] = sum(\n                len(r.get('sequence', '')) for r in generation_results\n            ) / len(generation_results)\n            results[max_length] = metrics\n            \n            print(f\"Length {max_length}: {metrics['elapsed_time']:.2f}s, \"\n                  f\"avg actual length: {metrics['avg_actual_length']:.1f}\")\n        \n        # Verify that longer sequences take more time\n        assert results[32]['elapsed_time'] < results[256]['elapsed_time']\n    \n    def test_temperature_performance_impact(self, base_config):\n        \"\"\"Test performance impact of different temperature settings.\"\"\"\n        temperatures = [0.5, 1.0, 1.5, 2.0]\n        results = {}\n        \n        diffuser = ProteinDiffuser(base_config)\n        \n        for temperature in temperatures:\n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            generation_results = diffuser.generate(\n                num_samples=5,\n                max_length=64,\n                temperature=temperature,\n                progress=False\n            )\n            \n            metrics = monitor.stop()\n            results[temperature] = metrics\n            \n            print(f\"Temperature {temperature}: {metrics['elapsed_time']:.2f}s\")\n        \n        # Temperature shouldn't dramatically affect performance\n        times = [results[t]['elapsed_time'] for t in temperatures]\n        assert max(times) / min(times) < 2.0  # Less than 2x difference\n    \n    @pytest.mark.slow\n    def test_memory_usage_stability(self, base_config):\n        \"\"\"Test memory usage stability over multiple generations.\"\"\"\n        diffuser = ProteinDiffuser(base_config)\n        memory_readings = []\n        \n        for i in range(10):\n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            generation_results = diffuser.generate(\n                num_samples=3,\n                max_length=32,\n                progress=False\n            )\n            \n            metrics = monitor.stop()\n            memory_readings.append(metrics['final_memory_mb'])\n            \n            # Small delay to allow garbage collection\n            time.sleep(0.1)\n        \n        # Memory usage should not grow significantly over time\n        initial_memory = memory_readings[0]\n        final_memory = memory_readings[-1]\n        memory_growth = final_memory - initial_memory\n        \n        print(f\"Memory growth over 10 iterations: {memory_growth:.1f}MB\")\n        assert memory_growth < 100, f\"Memory grew by {memory_growth:.1f}MB - potential leak\"\n    \n    def test_concurrent_generation_performance(self, base_config):\n        \"\"\"Test performance of concurrent generation requests.\"\"\"\n        diffuser = ProteinDiffuser(base_config)\n        \n        def single_generation():\n            return diffuser.generate(\n                num_samples=2,\n                max_length=32,\n                progress=False\n            )\n        \n        # Sequential execution\n        sequential_start = time.time()\n        for _ in range(4):\n            single_generation()\n        sequential_time = time.time() - sequential_start\n        \n        # Concurrent execution\n        concurrent_start = time.time()\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            futures = [executor.submit(single_generation) for _ in range(4)]\n            results = [future.result() for future in as_completed(futures)]\n        concurrent_time = time.time() - concurrent_start\n        \n        print(f\"Sequential: {sequential_time:.2f}s, Concurrent: {concurrent_time:.2f}s\")\n        \n        # Concurrent should be faster (or at least not much slower)\n        assert concurrent_time <= sequential_time * 1.5\n        assert len(results) == 4\n\n\n@pytest.mark.benchmark\nclass TestRankingPerformance:\n    \"\"\"Benchmark protein ranking performance.\"\"\"\n    \n    @pytest.fixture\n    def test_sequences(self):\n        \"\"\"Generate test sequences of varying lengths.\"\"\"\n        sequences = []\n        \n        # Short sequences\n        sequences.extend([\"ACDEFGHIKLMNPQRSTVWY\" for _ in range(10)])\n        \n        # Medium sequences\n        sequences.extend([\"MKLLILTCLVAVALARPKHPIPWDQAITVAYASRAL\" for _ in range(10)])\n        \n        # Long sequences\n        long_seq = \"MKLLILTCLVAVALARPKHPIPWDQAITVAYASRALGRGLVVMAQDGNRGGKFHPWTVNQGPLKDYICQAYDMGTTTEVPGTMGMLRRRSNVWSCLPRLLCERVAAPNLDPEGFVVAVPIPVYEAWDFGDPKLNLRQNTVAVTCTGVQTLAVRGRVGNLLSNGVPIGRGLPHIPSKGSGATFEFIGSDLKAELATDQAGVLQVDVQQVEACWFASQGGGVDTDYTGQPWDGGKPTVTGAMCGAFSCRHDGKRDVRVGTAAGVGGGYCSDGDGPVKPVVSNPNQALAFGLSEAGSRRLHPFTTARQGAGSM\"\n        sequences.extend([long_seq for _ in range(5)])\n        \n        return sequences\n    \n    def test_ranking_speed_scaling(self, test_sequences):\n        \"\"\"Test how ranking speed scales with number of sequences.\"\"\"\n        sequence_counts = [5, 10, 25, 50]\n        results = {}\n        \n        ranker = AffinityRanker()\n        \n        for count in sequence_counts:\n            test_subset = test_sequences[:count]\n            \n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            ranked_results = ranker.rank(test_subset, return_detailed=True)\n            \n            metrics = monitor.stop()\n            metrics['sequences_ranked'] = len(ranked_results)\n            metrics['time_per_sequence'] = metrics['elapsed_time'] / count\n            results[count] = metrics\n            \n            print(f\"Ranked {count} sequences in {metrics['elapsed_time']:.2f}s \"\n                  f\"({metrics['time_per_sequence']:.3f}s per sequence)\")\n        \n        # Verify reasonable scaling\n        assert results[5]['elapsed_time'] < results[50]['elapsed_time']\n        \n        # Time per sequence should remain reasonable\n        for count, metrics in results.items():\n            assert metrics['time_per_sequence'] < 1.0  # Less than 1 second per sequence\n    \n    def test_similarity_calculation_performance(self, test_sequences):\n        \"\"\"Test performance of similarity calculations.\"\"\"\n        from src.protein_diffusion.ranker import SequenceSimilarity\n        \n        # Test pairwise similarity calculations\n        monitor = PerformanceMonitor()\n        monitor.start()\n        \n        similarities = []\n        for i in range(min(20, len(test_sequences))):\n            for j in range(i + 1, min(20, len(test_sequences))):\n                sim = SequenceSimilarity.sequence_identity(\n                    test_sequences[i],\n                    test_sequences[j]\n                )\n                similarities.append(sim)\n        \n        metrics = monitor.stop()\n        \n        print(f\"Calculated {len(similarities)} pairwise similarities in {metrics['elapsed_time']:.2f}s\")\n        \n        # Should be fast for reasonable number of sequences\n        assert metrics['elapsed_time'] < 5.0\n        assert len(similarities) > 0\n    \n    def test_diversity_calculation_performance(self, test_sequences):\n        \"\"\"Test performance of diversity score calculation.\"\"\"\n        from src.protein_diffusion.ranker import SequenceSimilarity\n        \n        sequence_sets = [\n            test_sequences[:10],\n            test_sequences[:25],\n            test_sequences[:50] if len(test_sequences) >= 50 else test_sequences\n        ]\n        \n        for i, seq_set in enumerate(sequence_sets):\n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            diversity = SequenceSimilarity.calculate_diversity_score(seq_set)\n            \n            metrics = monitor.stop()\n            \n            print(f\"Diversity for {len(seq_set)} sequences: {diversity:.3f} \"\n                  f\"(calculated in {metrics['elapsed_time']:.2f}s)\")\n            \n            # Should complete in reasonable time\n            assert metrics['elapsed_time'] < 10.0\n            assert 0.0 <= diversity <= 1.0\n    \n    def test_ranking_with_quality_filters(self, test_sequences):\n        \"\"\"Test performance impact of quality filtering.\"\"\"\n        # Test with strict filters\n        strict_config = AffinityRankerConfig()\n        strict_config.min_confidence = 0.8\n        strict_config.min_structure_quality = 0.7\n        \n        # Test with lenient filters\n        lenient_config = AffinityRankerConfig()\n        lenient_config.min_confidence = 0.1\n        lenient_config.min_structure_quality = 0.1\n        \n        test_subset = test_sequences[:20]\n        \n        for config_name, config in [('strict', strict_config), ('lenient', lenient_config)]:\n            monitor = PerformanceMonitor()\n            monitor.start()\n            \n            ranker = AffinityRanker(config)\n            ranked_results = ranker.rank(test_subset, return_detailed=True)\n            \n            metrics = monitor.stop()\n            \n            print(f\"Ranking with {config_name} filters: {len(ranked_results)} results \"\n                  f\"in {metrics['elapsed_time']:.2f}s\")\n            \n            # Both should complete reasonably quickly\n            assert metrics['elapsed_time'] < 30.0\n\n\n@pytest.mark.benchmark\nclass TestSystemIntegrationPerformance:\n    \"\"\"Test performance of integrated system workflows.\"\"\"\n    \n    def test_full_pipeline_performance(self):\n        \"\"\"Test performance of complete generation -> ranking pipeline.\"\"\"\n        monitor = PerformanceMonitor()\n        monitor.start()\n        \n        # 1. Generation phase\n        config = ProteinDiffuserConfig()\n        config.num_samples = 10\n        config.max_length = 64\n        \n        diffuser = ProteinDiffuser(config)\n        generation_results = diffuser.generate(\n            motif=\"HELIX_SHEET\",\n            num_samples=10,\n            max_length=64,\n            progress=False\n        )\n        \n        generation_time = time.time()\n        \n        # 2. Ranking phase\n        sequences = [r['sequence'] for r in generation_results if r.get('sequence')]\n        ranker = AffinityRanker()\n        ranked_results = ranker.rank(sequences, return_detailed=True)\n        \n        final_metrics = monitor.stop()\n        \n        print(f\"Full pipeline: Generated {len(generation_results)} sequences, \"\n              f\"ranked {len(ranked_results)} in {final_metrics['elapsed_time']:.2f}s\")\n        \n        # Pipeline should complete in reasonable time\n        assert final_metrics['elapsed_time'] < 120.0  # 2 minutes max\n        assert len(ranked_results) > 0\n    \n    def test_batch_processing_performance(self):\n        \"\"\"Test performance of batch processing capabilities.\"\"\"\n        config = ProteinDiffuserConfig()\n        diffuser = ProteinDiffuser(config)\n        \n        # Create batch requests\n        batch_requests = [\n            {'motif': 'HELIX', 'num_samples': 3, 'max_length': 32},\n            {'motif': 'SHEET', 'num_samples': 3, 'max_length': 32},\n            {'motif': 'LOOP', 'num_samples': 3, 'max_length': 32},\n            {'motif': None, 'num_samples': 3, 'max_length': 32}\n        ]\n        \n        monitor = PerformanceMonitor()\n        monitor.start()\n        \n        batch_results = diffuser.generate_batch(batch_requests, progress=False)\n        \n        metrics = monitor.stop()\n        \n        total_sequences = sum(len(results) for results in batch_results)\n        \n        print(f\"Batch processing: {len(batch_requests)} requests, \"\n              f\"{total_sequences} sequences in {metrics['elapsed_time']:.2f}s\")\n        \n        # Batch processing should be efficient\n        assert metrics['elapsed_time'] < 60.0\n        assert len(batch_results) == len(batch_requests)\n        assert total_sequences > 0\n    \n    @pytest.mark.slow\n    def test_stress_test_sustained_load(self):\n        \"\"\"Stress test with sustained load over time.\"\"\"\n        config = ProteinDiffuserConfig()\n        config.num_samples = 5\n        config.max_length = 32\n        \n        diffuser = ProteinDiffuser(config)\n        ranker = AffinityRanker()\n        \n        iterations = 20\n        performance_data = []\n        \n        for i in range(iterations):\n            iteration_start = time.time()\n            \n            # Generate\n            results = diffuser.generate(\n                num_samples=5,\n                max_length=32,\n                progress=False\n            )\n            \n            # Rank\n            sequences = [r['sequence'] for r in results if r.get('sequence')]\n            if sequences:\n                ranked = ranker.get_top_candidates(sequences[:3], top_k=3)\n            \n            iteration_time = time.time() - iteration_start\n            performance_data.append(iteration_time)\n            \n            if i % 5 == 0:\n                print(f\"Iteration {i+1}/{iterations}: {iteration_time:.2f}s\")\n        \n        # Analyze performance stability\n        avg_time = statistics.mean(performance_data)\n        std_time = statistics.stdev(performance_data)\n        \n        print(f\"\\nStress test results:\")\n        print(f\"Average time per iteration: {avg_time:.2f}s ± {std_time:.2f}s\")\n        print(f\"Min time: {min(performance_data):.2f}s\")\n        print(f\"Max time: {max(performance_data):.2f}s\")\n        \n        # Performance should remain stable\n        assert avg_time < 10.0  # Average under 10 seconds\n        assert std_time < avg_time  # Standard deviation shouldn't exceed average\n        assert max(performance_data) < avg_time * 3  # No outliers more than 3x average\n    \n    def test_memory_efficiency_large_batch(self):\n        \"\"\"Test memory efficiency with large batch processing.\"\"\"\n        config = ProteinDiffuserConfig()\n        diffuser = ProteinDiffuser(config)\n        \n        # Monitor memory during large batch\n        monitor = PerformanceMonitor()\n        monitor.start()\n        \n        # Create larger batch\n        large_batch = [\n            {'num_samples': 2, 'max_length': 32}\n            for _ in range(10)\n        ]\n        \n        batch_results = diffuser.generate_batch(large_batch, progress=False)\n        \n        metrics = monitor.stop()\n        \n        print(f\"Large batch memory usage: {metrics['memory_delta_mb']:.1f}MB\")\n        print(f\"Final memory: {metrics['final_memory_mb']:.1f}MB\")\n        \n        # Memory usage should be reasonable\n        assert metrics['memory_delta_mb'] < 1000  # Less than 1GB increase\n        assert len(batch_results) == len(large_batch)\n\n\nif __name__ == '__main__':\n    # Run benchmarks when executed directly\n    pytest.main([__file__, '-v', '-m', 'benchmark'])"