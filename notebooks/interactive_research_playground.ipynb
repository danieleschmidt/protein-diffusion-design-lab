{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ Protein Diffusion Design Lab - Interactive Research Playground\n",
    "\n",
    "This notebook provides an interactive environment for exploring protein generation, analysis, and optimization using the Protein Diffusion Design Lab.\n",
    "\n",
    "## üìö Contents\n",
    "1. [System Setup & Health Check](#setup)\n",
    "2. [Basic Protein Generation](#generation)\n",
    "3. [Advanced Generation Parameters](#advanced)\n",
    "4. [Protein Ranking & Analysis](#ranking)\n",
    "5. [Research Experiments](#experiments)\n",
    "6. [Visualization & Insights](#visualization)\n",
    "7. [Performance Benchmarking](#benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import protein diffusion modules\n",
    "from protein_diffusion import (\n",
    "    ProteinDiffuser, ProteinDiffuserConfig,\n",
    "    AffinityRanker, AffinityRankerConfig,\n",
    "    StructurePredictor, StructurePredictorConfig\n",
    ")\n",
    "\n",
    "print(\"üß¨ Protein Diffusion Design Lab - Interactive Research Environment\")\n",
    "print(\"=" * 60)\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. üîß System Setup & Health Check\n",
    "\n",
    "Let's start by setting up our system and checking that everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system configuration\n",
    "config = ProteinDiffuserConfig()\n",
    "config.num_samples = 10\n",
    "config.max_length = 128\n",
    "config.temperature = 1.0\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"  ‚Ä¢ Samples per generation: {config.num_samples}\")\n",
    "print(f\"  ‚Ä¢ Maximum sequence length: {config.max_length}\")\n",
    "print(f\"  ‚Ä¢ Sampling temperature: {config.temperature}\")\n",
    "print(f\"  ‚Ä¢ Device: {config.device}\")\n",
    "print(f\"  ‚Ä¢ Data type: {config.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the protein diffuser\n",
    "print(\"üîÑ Initializing Protein Diffuser...\")\n",
    "diffuser = ProteinDiffuser(config)\n",
    "\n",
    "# Perform health check\n",
    "health_status = diffuser.health_check()\n",
    "\n",
    "print(\"\\nüè• System Health Check:\")\n",
    "print(f\"  Overall Status: {health_status['overall_status'].upper()}\")\n",
    "\n",
    "for component, status in health_status['components'].items():\n",
    "    status_icon = \"‚úÖ\" if status.get('status') == 'healthy' else \"‚ö†Ô∏è\" if status.get('status') == 'warning' else \"‚ùå\"\n",
    "    print(f\"  {status_icon} {component.title()}: {status.get('status', 'unknown')}\")\n",
    "\n",
    "if health_status.get('warnings'):\n",
    "    print(\"\\n‚ö†Ô∏è Warnings:\")\n",
    "    for warning in health_status['warnings']:\n",
    "        print(f\"  ‚Ä¢ {warning}\")\n",
    "\n",
    "if health_status.get('errors'):\n",
    "    print(\"\\n‚ùå Errors:\")\n",
    "    for error in health_status['errors']:\n",
    "        print(f\"  ‚Ä¢ {error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generation\"></a>\n",
    "## 2. üî¨ Basic Protein Generation\n",
    "\n",
    "Let's generate our first set of protein scaffolds and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate protein scaffolds with a simple motif\n",
    "print(\"üß¨ Generating protein scaffolds...\")\n",
    "print(\"Motif: HELIX_SHEET_HELIX\")\n",
    "print(f\"Samples: {config.num_samples}\")\n",
    "\n",
    "start_time = time.time()\n",
    "results = diffuser.generate(\n",
    "    motif=\"HELIX_SHEET_HELIX\",\n",
    "    num_samples=config.num_samples,\n",
    "    max_length=config.max_length,\n",
    "    temperature=config.temperature,\n",
    "    progress=False\n",
    ")\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Generation complete in {generation_time:.2f} seconds!\")\n",
    "print(f\"Generated {len(results)} sequences\")\n",
    "\n",
    "# Store results for later use\n",
    "session_results = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the generated sequences\n",
    "sequences = [r.get('sequence', '') for r in results if r.get('sequence')]\n",
    "confidences = [r.get('confidence', 0) for r in results]\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "print(\"üìä Generation Summary:\")\n",
    "print(f\"  ‚Ä¢ Total sequences: {len(sequences)}\")\n",
    "print(f\"  ‚Ä¢ Average length: {np.mean(lengths):.1f} ¬± {np.std(lengths):.1f}\")\n",
    "print(f\"  ‚Ä¢ Length range: {min(lengths)} - {max(lengths)}\")\n",
    "print(f\"  ‚Ä¢ Average confidence: {np.mean(confidences):.3f} ¬± {np.std(confidences):.3f}\")\n",
    "print(f\"  ‚Ä¢ High confidence (>0.7): {sum(1 for c in confidences if c > 0.7)}/{len(confidences)}\")\n",
    "\n",
    "# Display first few sequences\n",
    "print(\"\\nüß¨ Sample Sequences:\")\n",
    "for i, (seq, conf) in enumerate(zip(sequences[:3], confidences[:3])):\n",
    "    print(f\"  {i+1}. [{conf:.3f}] {seq[:60]}{'...' if len(seq) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequence properties\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Length distribution\n",
    "axes[0, 0].hist(lengths, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Sequence Length Distribution')\n",
    "axes[0, 0].set_xlabel('Length (residues)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Confidence distribution\n",
    "axes[0, 1].hist(confidences, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('Confidence Score Distribution')\n",
    "axes[0, 1].set_xlabel('Confidence')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Length vs Confidence scatter\n",
    "axes[1, 0].scatter(lengths, confidences, alpha=0.6, color='mediumseagreen')\n",
    "axes[1, 0].set_title('Length vs Confidence')\n",
    "axes[1, 0].set_xlabel('Length (residues)')\n",
    "axes[1, 0].set_ylabel('Confidence')\n",
    "\n",
    "# Amino acid composition\n",
    "aa_counts = {}\n",
    "total_residues = 0\n",
    "for seq in sequences:\n",
    "    for aa in seq:\n",
    "        if aa.isalpha():\n",
    "            aa_counts[aa] = aa_counts.get(aa, 0) + 1\n",
    "            total_residues += 1\n",
    "\n",
    "aa_percentages = {aa: (count/total_residues)*100 for aa, count in aa_counts.items()}\n",
    "sorted_aa = sorted(aa_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if sorted_aa:\n",
    "    aas, percentages = zip(*sorted_aa[:10])  # Top 10 amino acids\n",
    "    axes[1, 1].bar(aas, percentages, color='gold', alpha=0.7)\n",
    "    axes[1, 1].set_title('Top 10 Amino Acid Frequencies')\n",
    "    axes[1, 1].set_xlabel('Amino Acid')\n",
    "    axes[1, 1].set_ylabel('Percentage (%)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Sequence analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced\"></a>\n",
    "## 3. üî¨ Advanced Generation Parameters\n",
    "\n",
    "Let's explore how different parameters affect protein generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different temperatures\n",
    "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "temp_results = {}\n",
    "\n",
    "print(\"üå°Ô∏è Temperature Experiment:\")\n",
    "print(\"Testing different sampling temperatures...\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nüî• Temperature: {temp}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    temp_gen_results = diffuser.generate(\n",
    "        motif=\"HELIX_LOOP_HELIX\",\n",
    "        num_samples=5,\n",
    "        max_length=64,\n",
    "        temperature=temp,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    temp_sequences = [r.get('sequence', '') for r in temp_gen_results if r.get('sequence')]\n",
    "    temp_confidences = [r.get('confidence', 0) for r in temp_gen_results]\n",
    "    temp_lengths = [len(seq) for seq in temp_sequences]\n",
    "    \n",
    "    temp_results[temp] = {\n",
    "        'sequences': temp_sequences,\n",
    "        'confidences': temp_confidences,\n",
    "        'lengths': temp_lengths,\n",
    "        'generation_time': gen_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Generated: {len(temp_sequences)} sequences\")\n",
    "    print(f\"  Avg confidence: {np.mean(temp_confidences):.3f}\")\n",
    "    print(f\"  Avg length: {np.mean(temp_lengths):.1f}\")\n",
    "    print(f\"  Time: {gen_time:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Temperature experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "temps = list(temp_results.keys())\n",
    "avg_confidences = [np.mean(temp_results[t]['confidences']) for t in temps]\n",
    "avg_lengths = [np.mean(temp_results[t]['lengths']) for t in temps]\n",
    "gen_times = [temp_results[t]['generation_time'] for t in temps]\n",
    "\n",
    "# Confidence vs Temperature\n",
    "axes[0].plot(temps, avg_confidences, 'o-', color='red', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Average Confidence vs Temperature')\n",
    "axes[0].set_xlabel('Temperature')\n",
    "axes[0].set_ylabel('Average Confidence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Length vs Temperature\n",
    "axes[1].plot(temps, avg_lengths, 's-', color='blue', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Average Length vs Temperature')\n",
    "axes[1].set_xlabel('Temperature')\n",
    "axes[1].set_ylabel('Average Length (residues)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Generation Time vs Temperature\n",
    "axes[2].plot(temps, gen_times, '^-', color='green', linewidth=2, markersize=8)\n",
    "axes[2].set_title('Generation Time vs Temperature')\n",
    "axes[2].set_xlabel('Temperature')\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different motif types\n",
    "motifs_to_test = [\n",
    "    \"HELIX\",\n",
    "    \"SHEET\",\n",
    "    \"LOOP\",\n",
    "    \"HELIX_SHEET\",\n",
    "    \"SHEET_LOOP_SHEET\",\n",
    "    None  # No motif conditioning\n",
    "]\n",
    "\n",
    "motif_results = {}\n",
    "\n",
    "print(\"üß¨ Motif Experiment:\")\n",
    "print(\"Testing different structural motifs...\")\n",
    "\n",
    "for motif in motifs_to_test:\n",
    "    motif_name = motif or \"None\"\n",
    "    print(f\"\\nüèóÔ∏è Motif: {motif_name}\")\n",
    "    \n",
    "    motif_gen_results = diffuser.generate(\n",
    "        motif=motif,\n",
    "        num_samples=3,\n",
    "        max_length=48,\n",
    "        temperature=1.0,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    motif_sequences = [r.get('sequence', '') for r in motif_gen_results if r.get('sequence')]\n",
    "    motif_confidences = [r.get('confidence', 0) for r in motif_gen_results]\n",
    "    \n",
    "    motif_results[motif_name] = {\n",
    "        'sequences': motif_sequences,\n",
    "        'confidences': motif_confidences\n",
    "    }\n",
    "    \n",
    "    print(f\"  Generated: {len(motif_sequences)} sequences\")\n",
    "    print(f\"  Avg confidence: {np.mean(motif_confidences):.3f}\")\n",
    "    \n",
    "    # Show one example\n",
    "    if motif_sequences:\n",
    "        example = motif_sequences[0][:40] + ('...' if len(motif_sequences[0]) > 40 else '')\n",
    "        print(f\"  Example: {example}\")\n",
    "\n",
    "print(\"\\n‚úÖ Motif experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ranking\"></a>\n",
    "## 4. üèÜ Protein Ranking & Analysis\n",
    "\n",
    "Now let's rank and analyze our generated proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ranking system\n",
    "ranker_config = AffinityRankerConfig()\n",
    "ranker_config.binding_weight = 0.4\n",
    "ranker_config.structure_weight = 0.3\n",
    "ranker_config.diversity_weight = 0.2\n",
    "ranker_config.novelty_weight = 0.1\n",
    "ranker_config.max_results = 50\n",
    "\n",
    "print(\"üèÜ Initializing Protein Ranking System\")\n",
    "print(f\"  ‚Ä¢ Binding weight: {ranker_config.binding_weight}\")\n",
    "print(f\"  ‚Ä¢ Structure weight: {ranker_config.structure_weight}\")\n",
    "print(f\"  ‚Ä¢ Diversity weight: {ranker_config.diversity_weight}\")\n",
    "print(f\"  ‚Ä¢ Novelty weight: {ranker_config.novelty_weight}\")\n",
    "\n",
    "ranker = AffinityRanker(ranker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sequences from our experiments\n",
    "all_sequences = []\n",
    "\n",
    "# Add sequences from initial generation\n",
    "initial_sequences = [r.get('sequence', '') for r in session_results if r.get('sequence')]\n",
    "all_sequences.extend(initial_sequences)\n",
    "\n",
    "# Add sequences from temperature experiments\n",
    "for temp, data in temp_results.items():\n",
    "    all_sequences.extend(data['sequences'])\n",
    "\n",
    "# Add sequences from motif experiments\n",
    "for motif, data in motif_results.items():\n",
    "    all_sequences.extend(data['sequences'])\n",
    "\n",
    "# Remove duplicates\n",
    "unique_sequences = list(set(all_sequences))\n",
    "print(f\"üß¨ Total unique sequences for ranking: {len(unique_sequences)}\")\n",
    "\n",
    "# Rank the sequences\n",
    "print(\"\\nüîÑ Ranking sequences...\")\n",
    "start_time = time.time()\n",
    "\n",
    "ranked_results = ranker.rank(\n",
    "    unique_sequences[:20],  # Rank top 20 for demo\n",
    "    return_detailed=True\n",
    ")\n",
    "\n",
    "ranking_time = time.time() - start_time\n",
    "print(f\"‚úÖ Ranking complete in {ranking_time:.2f} seconds!\")\n",
    "print(f\"Ranked {len(ranked_results)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ranking results\n",
    "if ranked_results:\n",
    "    print(\"üèÖ Top 10 Ranked Sequences:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(ranked_results[:10]):\n",
    "        sequence = result['sequence']\n",
    "        score = result.get('composite_score', 0)\n",
    "        binding = result.get('binding_affinity', 0)\n",
    "        structure = result.get('structure_quality', 0)\n",
    "        confidence = result.get('confidence', 0)\n",
    "        \n",
    "        print(f\"{i+1:2d}. Score: {score:.4f} | Binding: {binding:6.2f} | \"\n",
    "              f\"Structure: {structure:.3f} | Confidence: {confidence:.3f}\")\n",
    "        print(f\"    {sequence[:60]}{'...' if len(sequence) > 60 else ''}\")\n",
    "        print()\n",
    "    \n",
    "    # Get ranking statistics\n",
    "    ranking_stats = ranker.get_ranking_statistics(ranked_results)\n",
    "    \n",
    "    print(\"üìä Ranking Statistics:\")\n",
    "    for key, value in ranking_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No sequences passed ranking filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ranking results\n",
    "if ranked_results and len(ranked_results) > 5:\n",
    "    # Extract data for plotting\n",
    "    ranks = list(range(1, len(ranked_results) + 1))\n",
    "    composite_scores = [r.get('composite_score', 0) for r in ranked_results]\n",
    "    binding_affinities = [r.get('binding_affinity', 0) for r in ranked_results]\n",
    "    structure_qualities = [r.get('structure_quality', 0) for r in ranked_results]\n",
    "    confidences = [r.get('confidence', 0) for r in ranked_results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Composite scores vs rank\n",
    "    axes[0, 0].plot(ranks, composite_scores, 'o-', color='purple', linewidth=2)\n",
    "    axes[0, 0].set_title('Composite Scores by Rank')\n",
    "    axes[0, 0].set_xlabel('Rank')\n",
    "    axes[0, 0].set_ylabel('Composite Score')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Score distribution histogram\n",
    "    axes[0, 1].hist(composite_scores, bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[0, 1].set_title('Composite Score Distribution')\n",
    "    axes[0, 1].set_xlabel('Composite Score')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Binding affinity vs structure quality\n",
    "    scatter = axes[1, 0].scatter(binding_affinities, structure_qualities, \n",
    "                                c=composite_scores, cmap='viridis', alpha=0.7, s=60)\n",
    "    axes[1, 0].set_title('Binding Affinity vs Structure Quality')\n",
    "    axes[1, 0].set_xlabel('Binding Affinity (kcal/mol)')\n",
    "    axes[1, 0].set_ylabel('Structure Quality')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Composite Score')\n",
    "    \n",
    "    # Multi-metric radar chart for top sequence\n",
    "    if ranked_results:\n",
    "        top_result = ranked_results[0]\n",
    "        metrics = ['Composite\\nScore', 'Binding\\nAffinity', 'Structure\\nQuality', 'Confidence']\n",
    "        values = [\n",
    "            top_result.get('composite_score', 0),\n",
    "            abs(top_result.get('binding_affinity', 0)) / 20,  # Normalize\n",
    "            top_result.get('structure_quality', 0),\n",
    "            top_result.get('confidence', 0)\n",
    "        ]\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        values += values[:1]  # Complete the circle\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        axes[1, 1] = plt.subplot(2, 2, 4, projection='polar')\n",
    "        axes[1, 1].plot(angles, values, 'o-', linewidth=2, color='red')\n",
    "        axes[1, 1].fill(angles, values, alpha=0.25, color='red')\n",
    "        axes[1, 1].set_xticks(angles[:-1])\n",
    "        axes[1, 1].set_xticklabels(metrics)\n",
    "        axes[1, 1].set_title('Top Sequence Metrics', y=1.08)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Ranking visualization complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for ranking visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"experiments\"></a>\n",
    "## 5. üß™ Research Experiments\n",
    "\n",
    "Let's conduct some research experiments to explore protein design space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Motif-specific generation study\n",
    "print(\"üß™ Experiment 1: Motif-Specific Generation Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define experimental conditions\n",
    "experimental_motifs = [\n",
    "    (\"HELIX\", \"Alpha helix - basic secondary structure\"),\n",
    "    (\"SHEET\", \"Beta sheet - extended structure\"),\n",
    "    (\"HELIX_SHEET_HELIX\", \"Mixed motif - helix-sheet-helix\"),\n",
    "    (\"SHEET_LOOP_SHEET\", \"Mixed motif - sheet-loop-sheet\")\n",
    "]\n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "for motif, description in experimental_motifs:\n",
    "    print(f\"\\nüî¨ Testing: {motif}\")\n",
    "    print(f\"  Description: {description}\")\n",
    "    \n",
    "    # Generate samples with this motif\n",
    "    exp_results = diffuser.generate(\n",
    "        motif=motif,\n",
    "        num_samples=8,\n",
    "        max_length=80,\n",
    "        temperature=1.0,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    # Analyze sequences\n",
    "    exp_sequences = [r.get('sequence', '') for r in exp_results if r.get('sequence')]\n",
    "    exp_confidences = [r.get('confidence', 0) for r in exp_results]\n",
    "    exp_lengths = [len(seq) for seq in exp_sequences]\n",
    "    \n",
    "    # Calculate amino acid preferences\n",
    "    aa_composition = {}\n",
    "    total_residues = sum(len(seq) for seq in exp_sequences)\n",
    "    \n",
    "    for seq in exp_sequences:\n",
    "        for aa in seq:\n",
    "            if aa.isalpha():\n",
    "                aa_composition[aa] = aa_composition.get(aa, 0) + 1\n",
    "    \n",
    "    aa_frequencies = {aa: count/total_residues for aa, count in aa_composition.items()}\n",
    "    \n",
    "    experiment_results[motif] = {\n",
    "        'sequences': exp_sequences,\n",
    "        'confidences': exp_confidences,\n",
    "        'lengths': exp_lengths,\n",
    "        'aa_frequencies': aa_frequencies,\n",
    "        'description': description\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ Generated {len(exp_sequences)} sequences\")\n",
    "    print(f\"  üìè Avg length: {np.mean(exp_lengths):.1f} ¬± {np.std(exp_lengths):.1f}\")\n",
    "    print(f\"  üéØ Avg confidence: {np.mean(exp_confidences):.3f} ¬± {np.std(exp_confidences):.3f}\")\n",
    "    \n",
    "    # Top 3 most frequent amino acids\n",
    "    top_aa = sorted(aa_frequencies.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"  üß¨ Top AAs: {', '.join([f'{aa}({freq:.1%})' for aa, freq in top_aa])}\")\n",
    "\n",
    "print(\"\\n‚úÖ Motif experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize motif experiment results\n",
    "motif_names = list(experiment_results.keys())\n",
    "n_motifs = len(motif_names)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Confidence comparison\n",
    "conf_data = [experiment_results[motif]['confidences'] for motif in motif_names]\n",
    "axes[0, 0].boxplot(conf_data, labels=[m.replace('_', '\\n') for m in motif_names])\n",
    "axes[0, 0].set_title('Confidence Distributions by Motif')\n",
    "axes[0, 0].set_ylabel('Confidence Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Length comparison\n",
    "length_data = [experiment_results[motif]['lengths'] for motif in motif_names]\n",
    "axes[0, 1].boxplot(length_data, labels=[m.replace('_', '\\n') for m in motif_names])\n",
    "axes[0, 1].set_title('Length Distributions by Motif')\n",
    "axes[0, 1].set_ylabel('Sequence Length')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Amino acid frequency heatmap\n",
    "all_aas = set()\n",
    "for motif_data in experiment_results.values():\n",
    "    all_aas.update(motif_data['aa_frequencies'].keys())\n",
    "\n",
    "all_aas = sorted(list(all_aas))\n",
    "freq_matrix = []\n",
    "\n",
    "for motif in motif_names:\n",
    "    motif_freqs = experiment_results[motif]['aa_frequencies']\n",
    "    freq_row = [motif_freqs.get(aa, 0) for aa in all_aas]\n",
    "    freq_matrix.append(freq_row)\n",
    "\n",
    "if freq_matrix and all_aas:\n",
    "    im = axes[1, 0].imshow(freq_matrix, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_xticks(range(len(all_aas)))\n",
    "    axes[1, 0].set_xticklabels(all_aas, rotation=45)\n",
    "    axes[1, 0].set_yticks(range(len(motif_names)))\n",
    "    axes[1, 0].set_yticklabels([m.replace('_', ' ') for m in motif_names])\n",
    "    axes[1, 0].set_title('Amino Acid Frequencies by Motif')\n",
    "    plt.colorbar(im, ax=axes[1, 0], label='Frequency')\n",
    "\n",
    "# 4. Sample diversity analysis\n",
    "from src.protein_diffusion.ranker import SequenceSimilarity\n",
    "\n",
    "diversity_scores = []\n",
    "for motif in motif_names:\n",
    "    sequences = experiment_results[motif]['sequences']\n",
    "    diversity = SequenceSimilarity.calculate_diversity_score(sequences)\n",
    "    diversity_scores.append(diversity)\n",
    "\n",
    "bars = axes[1, 1].bar(range(len(motif_names)), diversity_scores, \n",
    "                     color=['red', 'blue', 'green', 'orange'][:len(motif_names)], alpha=0.7)\n",
    "axes[1, 1].set_xticks(range(len(motif_names)))\n",
    "axes[1, 1].set_xticklabels([m.replace('_', '\\n') for m in motif_names], rotation=45)\n",
    "axes[1, 1].set_title('Sequence Diversity by Motif')\n",
    "axes[1, 1].set_ylabel('Diversity Score')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, diversity_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Motif analysis visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Temperature vs Quality Trade-off Study\n",
    "print(\"\\nüß™ Experiment 2: Temperature vs Quality Trade-off\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extended temperature range\n",
    "temp_range = [0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 1.8, 2.0]\n",
    "quality_metrics = {\n",
    "    'temperatures': [],\n",
    "    'avg_confidence': [],\n",
    "    'diversity_scores': [],\n",
    "    'avg_length': [],\n",
    "    'generation_success_rate': []\n",
    "}\n",
    "\n",
    "print(\"üå°Ô∏è Testing temperature range:\", temp_range)\n",
    "\n",
    "for temp in temp_range:\n",
    "    print(f\"\\nüî• Temperature: {temp}\")\n",
    "    \n",
    "    temp_exp_results = diffuser.generate(\n",
    "        motif=\"HELIX_SHEET\",\n",
    "        num_samples=6,\n",
    "        max_length=64,\n",
    "        temperature=temp,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    # Extract successful results\n",
    "    successful_results = [r for r in temp_exp_results if r.get('sequence') and len(r['sequence']) > 10]\n",
    "    success_rate = len(successful_results) / len(temp_exp_results)\n",
    "    \n",
    "    if successful_results:\n",
    "        temp_sequences = [r['sequence'] for r in successful_results]\n",
    "        temp_confidences = [r['confidence'] for r in successful_results]\n",
    "        temp_lengths = [len(seq) for seq in temp_sequences]\n",
    "        \n",
    "        # Calculate diversity\n",
    "        diversity = SequenceSimilarity.calculate_diversity_score(temp_sequences)\n",
    "        \n",
    "        quality_metrics['temperatures'].append(temp)\n",
    "        quality_metrics['avg_confidence'].append(np.mean(temp_confidences))\n",
    "        quality_metrics['diversity_scores'].append(diversity)\n",
    "        quality_metrics['avg_length'].append(np.mean(temp_lengths))\n",
    "        quality_metrics['generation_success_rate'].append(success_rate)\n",
    "        \n",
    "        print(f\"  ‚úÖ Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  üéØ Avg confidence: {np.mean(temp_confidences):.3f}\")\n",
    "        print(f\"  üåà Diversity: {diversity:.3f}\")\n",
    "        print(f\"  üìè Avg length: {np.mean(temp_lengths):.1f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå No successful generations\")\n",
    "\n",
    "print(\"\\n‚úÖ Temperature-quality experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature vs quality trade-offs\n",
    "if quality_metrics['temperatures']:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    temps = quality_metrics['temperatures']\n",
    "    \n",
    "    # Confidence vs Temperature\n",
    "    axes[0, 0].plot(temps, quality_metrics['avg_confidence'], 'o-', \n",
    "                   color='red', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_title('Average Confidence vs Temperature')\n",
    "    axes[0, 0].set_xlabel('Temperature')\n",
    "    axes[0, 0].set_ylabel('Average Confidence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Diversity vs Temperature\n",
    "    axes[0, 1].plot(temps, quality_metrics['diversity_scores'], 's-', \n",
    "                   color='blue', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_title('Sequence Diversity vs Temperature')\n",
    "    axes[0, 1].set_xlabel('Temperature')\n",
    "    axes[0, 1].set_ylabel('Diversity Score')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Success Rate vs Temperature\n",
    "    axes[1, 0].plot(temps, quality_metrics['generation_success_rate'], '^-', \n",
    "                   color='green', linewidth=2, markersize=8)\n",
    "    axes[1, 0].set_title('Generation Success Rate vs Temperature')\n",
    "    axes[1, 0].set_xlabel('Temperature')\n",
    "    axes[1, 0].set_ylabel('Success Rate')\n",
    "    axes[1, 0].set_ylim(0, 1.1)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence vs Diversity scatter (trade-off analysis)\n",
    "    scatter = axes[1, 1].scatter(quality_metrics['avg_confidence'], \n",
    "                               quality_metrics['diversity_scores'],\n",
    "                               c=temps, cmap='plasma', s=100, alpha=0.8)\n",
    "    axes[1, 1].set_title('Confidence vs Diversity Trade-off')\n",
    "    axes[1, 1].set_xlabel('Average Confidence')\n",
    "    axes[1, 1].set_ylabel('Diversity Score')\n",
    "    plt.colorbar(scatter, ax=axes[1, 1], label='Temperature')\n",
    "    \n",
    "    # Annotate points with temperature values\n",
    "    for i, temp in enumerate(temps):\n",
    "        axes[1, 1].annotate(f'{temp}', \n",
    "                          (quality_metrics['avg_confidence'][i], \n",
    "                           quality_metrics['diversity_scores'][i]),\n",
    "                          xytext=(5, 5), textcoords='offset points', \n",
    "                          fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal temperature\n",
    "    # Balance confidence and diversity (equal weighting)\n",
    "    combined_scores = []\n",
    "    for i in range(len(temps)):\n",
    "        conf_norm = quality_metrics['avg_confidence'][i]\n",
    "        div_norm = quality_metrics['diversity_scores'][i]\n",
    "        combined_score = (conf_norm + div_norm) / 2\n",
    "        combined_scores.append(combined_score)\n",
    "    \n",
    "    optimal_idx = np.argmax(combined_scores)\n",
    "    optimal_temp = temps[optimal_idx]\n",
    "    \n",
    "    print(f\"üìà Temperature Analysis Results:\")\n",
    "    print(f\"  ‚Ä¢ Optimal temperature (balanced): {optimal_temp}\")\n",
    "    print(f\"  ‚Ä¢ Best confidence: {max(quality_metrics['avg_confidence']):.3f} (T={temps[np.argmax(quality_metrics['avg_confidence'])]})\")\n",
    "    print(f\"  ‚Ä¢ Best diversity: {max(quality_metrics['diversity_scores']):.3f} (T={temps[np.argmax(quality_metrics['diversity_scores'])]})\")\n",
    "    print(f\"  ‚Ä¢ Best success rate: {max(quality_metrics['generation_success_rate']):.1%} (T={temps[np.argmax(quality_metrics['generation_success_rate'])]})\")\nelse:\n    print(\"‚ö†Ô∏è No temperature data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benchmarks\"></a>\n",
    "## 6. ‚ö° Performance Benchmarking\n",
    "\n",
    "Let's benchmark the system performance and identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "print(\"‚ö° System Performance Benchmarks\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Benchmark 1: Generation speed scaling\n",
    "print(\"\\nüöÄ Benchmark 1: Generation Speed Scaling\")\n",
    "sample_counts = [1, 5, 10, 20]\n",
    "generation_benchmarks = []\n",
    "\n",
    "for n_samples in sample_counts:\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Measure memory before\n",
    "    process = psutil.Process()\n",
    "    memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Time generation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    benchmark_results = diffuser.generate(\n",
    "        num_samples=n_samples,\n",
    "        max_length=64,\n",
    "        temperature=1.0,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Measure memory after\n",
    "    memory_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    memory_used = memory_after - memory_before\n",
    "    \n",
    "    generation_time = end_time - start_time\n",
    "    time_per_sample = generation_time / n_samples\n",
    "    \n",
    "    generation_benchmarks.append({\n",
    "        'samples': n_samples,\n",
    "        'total_time': generation_time,\n",
    "        'time_per_sample': time_per_sample,\n",
    "        'memory_used': memory_used,\n",
    "        'sequences_generated': len([r for r in benchmark_results if r.get('sequence')])\n",
    "    })\n",
    "    \n",
    "    print(f\"  {n_samples:2d} samples: {generation_time:6.2f}s total, \"\n",
    "          f\"{time_per_sample:6.3f}s/sample, {memory_used:6.1f}MB\")\n",
    "\n",
    "print(\"\\nüèÜ Benchmark 2: Ranking Speed Scaling\")\n",
    "# Create test sequences of different counts\n",
    "test_sequences = [\"ACDEFGHIKLMNPQRSTVWY\" * (i % 3 + 1) for i in range(50)]\n",
    "sequence_counts = [5, 10, 20, 30]\n",
    "ranking_benchmarks = []\n",
    "\n",
    "for n_sequences in sequence_counts:\n",
    "    test_subset = test_sequences[:n_sequences]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ranking_results = ranker.rank(test_subset, return_detailed=False)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    ranking_time = end_time - start_time\n",
    "    time_per_sequence = ranking_time / n_sequences\n",
    "    \n",
    "    ranking_benchmarks.append({\n",
    "        'sequences': n_sequences,\n",
    "        'total_time': ranking_time,\n",
    "        'time_per_sequence': time_per_sequence,\n",
    "        'sequences_ranked': len(ranking_results)\n",
    "    })\n",
    "    \n",
    "    print(f\"  {n_sequences:2d} sequences: {ranking_time:6.2f}s total, \"\n",
    "          f\"{time_per_sequence:6.3f}s/seq, {len(ranking_results)} ranked\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance benchmarks\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Generation performance\n",
    "gen_samples = [b['samples'] for b in generation_benchmarks]\n",
    "gen_times = [b['total_time'] for b in generation_benchmarks]\n",
    "gen_times_per_sample = [b['time_per_sample'] for b in generation_benchmarks]\n",
    "gen_memory = [b['memory_used'] for b in generation_benchmarks]\n",
    "\n",
    "# Total generation time scaling\n",
    "axes[0, 0].plot(gen_samples, gen_times, 'o-', color='blue', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Total Generation Time Scaling')\n",
    "axes[0, 0].set_xlabel('Number of Samples')\n",
    "axes[0, 0].set_ylabel('Total Time (seconds)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time per sample efficiency\n",
    "axes[0, 1].plot(gen_samples, gen_times_per_sample, 's-', color='red', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Generation Efficiency')\n",
    "axes[0, 1].set_xlabel('Number of Samples')\n",
    "axes[0, 1].set_ylabel('Time per Sample (seconds)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage scaling\n",
    "axes[1, 0].plot(gen_samples, gen_memory, '^-', color='green', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_title('Memory Usage Scaling')\n",
    "axes[1, 0].set_xlabel('Number of Samples')\n",
    "axes[1, 0].set_ylabel('Memory Used (MB)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ranking performance\n",
    "rank_sequences = [b['sequences'] for b in ranking_benchmarks]\n",
    "rank_times = [b['total_time'] for b in ranking_benchmarks]\n",
    "\n",
    "axes[1, 1].plot(rank_sequences, rank_times, 'd-', color='purple', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_title('Ranking Time Scaling')\n",
    "axes[1, 1].set_xlabel('Number of Sequences')\n",
    "axes[1, 1].set_ylabel('Ranking Time (seconds)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"üìä Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Generation throughput: {gen_samples[-1] / gen_times[-1]:.1f} sequences/second\")\n",
    "print(f\"  ‚Ä¢ Ranking throughput: {rank_sequences[-1] / rank_times[-1]:.1f} sequences/second\")\n",
    "print(f\"  ‚Ä¢ Memory efficiency: {gen_memory[-1] / gen_samples[-1]:.1f} MB/sample\")\n",
    "\n",
    "# Identify bottlenecks\n",
    "if gen_times_per_sample[-1] > gen_times_per_sample[0] * 1.5:\n",
    "    print(\"  ‚ö†Ô∏è  Generation efficiency decreases with batch size\")\nelse:\n    print(\"  ‚úÖ Generation scales efficiently with batch size\")\n\nif rank_times[-1] / rank_sequences[-1] > rank_times[0] / rank_sequences[0] * 2:\n    print(\"  ‚ö†Ô∏è  Ranking performance degrades significantly with size\")\nelse:\n    print(\"  ‚úÖ Ranking performance scales reasonably\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Research Session Summary\n",
    "\n",
    "This interactive research session has demonstrated the capabilities of the Protein Diffusion Design Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive session report\n",
    "print(\"üìã Research Session Summary Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all statistics\n",
    "total_sequences_generated = len(unique_sequences) if 'unique_sequences' in locals() else 0\n",
    "total_sequences_ranked = len(ranked_results) if 'ranked_results' in locals() and ranked_results else 0\n",
    "\n",
    "print(f\"\\nüß¨ Generation Statistics:\")\n",
    "print(f\"  ‚Ä¢ Total unique sequences generated: {total_sequences_generated}\")\n",
    "print(f\"  ‚Ä¢ Motifs tested: {len(experimental_motifs) if 'experimental_motifs' in locals() else 0}\")\n",
    "print(f\"  ‚Ä¢ Temperature range explored: {min(temp_range) if 'temp_range' in locals() else 'N/A'} - {max(temp_range) if 'temp_range' in locals() else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nüèÜ Ranking & Analysis:\")\n",
    "print(f\"  ‚Ä¢ Sequences successfully ranked: {total_sequences_ranked}\")\n",
    "if 'ranked_results' in locals() and ranked_results:\n",
    "    top_score = ranked_results[0]['composite_score']\n",
    "    print(f\"  ‚Ä¢ Highest composite score achieved: {top_score:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "if 'generation_benchmarks' in locals():\n",
    "    best_throughput = max(b['samples'] / b['total_time'] for b in generation_benchmarks)\n",
    "    print(f\"  ‚Ä¢ Best generation throughput: {best_throughput:.1f} sequences/second\")\n",
    "if 'ranking_benchmarks' in locals():\n",
    "    best_ranking_throughput = max(b['sequences'] / b['total_time'] for b in ranking_benchmarks)\n",
    "    print(f\"  ‚Ä¢ Best ranking throughput: {best_ranking_throughput:.1f} sequences/second\")\n",
    "\n",
    "print(f\"\\nüß™ Research Insights:\")\nif 'experiment_results' in locals():\n    print(f\"  ‚Ä¢ Motif diversity analysis completed for {len(experiment_results)} motifs\")\n    \n    # Find most diverse motif\n    if 'diversity_scores' in locals() and diversity_scores:\n        most_diverse_idx = np.argmax(diversity_scores)\n        most_diverse_motif = motif_names[most_diverse_idx] if 'motif_names' in locals() else 'Unknown'\n        print(f\"  ‚Ä¢ Most diverse motif: {most_diverse_motif} (diversity: {max(diversity_scores):.3f})\")\n\nif 'quality_metrics' in locals() and quality_metrics['temperatures']:\n    print(f\"  ‚Ä¢ Optimal temperature for balanced performance: {optimal_temp if 'optimal_temp' in locals() else 'N/A'}\")\n\nprint(f\"\\n‚úÖ Session Status: COMPLETE\")\nprint(f\"üéØ System Health: {health_status['overall_status'].upper() if 'health_status' in locals() else 'UNKNOWN'}\")\n\n# Save session results if needed\nsession_timestamp = time.strftime('%Y%m%d_%H%M%S')\nprint(f\"\\nüíæ Session ID: research_session_{session_timestamp}\")\nprint(\"üìÅ All results are available in the notebook variables for further analysis.\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"üß¨ Thank you for using the Protein Diffusion Design Lab!\")\nprint(\"üî¨ Happy researching!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}